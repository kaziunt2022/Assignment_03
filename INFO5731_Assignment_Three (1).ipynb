{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USSdXHuqnwv9"
   },
   "source": [
    "# **INFO5731 Assignment Three**\n",
    "\n",
    "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment. (Use 500 records amount all the data you collected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWxodXh5n4xF"
   },
   "source": [
    "# **Question 1: Understand N-gram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TenBkDJ5n95k"
   },
   "source": [
    "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
    "\n",
    "(1) Count the frequency of all the N-grams (N=3).\n",
    "\n",
    "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
    "\n",
    "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as the 500 reviews (abstracts, or tweets). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuFPKhC0m1fd"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import nltk, string, re\n",
    "\n",
    "## Importing the data\n",
    "df = pd.read_csv(\"Reviews.CSV\", nrows=500) # import first 500 reviews\n",
    "all_reviews = list(df['Reviews']) # extracting reviews text\n",
    "\n",
    "## Tokenize the text of reviews\n",
    "review_tokens = [nltk.tokenize.casual_tokenize(i) for i in all_reviews]\n",
    "\n",
    "## Cleaning the data\n",
    "stopwords = nltk.corpus.stopwords.words('english') # get all stopwords\n",
    "punctuations = string.punctuation # get all punctuations\n",
    "clean_text = [j.lower() for i in review_tokens for j in i if ((j not in stopwords) and (j not in punctuations) and (j.isalpha()))] # removing all punctions, stopwords, and non alphabatic words\n",
    "\n",
    "## get 3 grams\n",
    "three_grams = nltk.ngrams(clean_text, 3)\n",
    "freq_three_word = {}\n",
    "# Count freq of each tri-gram\n",
    "for three_word in three_grams:\n",
    "    if three_word in freq_three_word:\n",
    "        freq_three_word[three_word]+=1\n",
    "    else:\n",
    "        freq_three_word[three_word]=1\n",
    "print(\"Part 1:\\n\", freq_three_word)\n",
    "print()\n",
    "\n",
    "## calculate probability\n",
    "# claculate frequency of each word\n",
    "freq_each_word = {}\n",
    "for word in clean_text:\n",
    "    if word in freq_each_word:\n",
    "        freq_each_word[word]+=1\n",
    "    else:\n",
    "        freq_each_word[word]=1\n",
    "# find 2 gram and calculate there frequency\n",
    "two_gram = nltk.ngrams(clean_text, 2)\n",
    "freq_two_word = {}\n",
    "for two_word in two_gram:\n",
    "    if two_word in freq_two_word:\n",
    "        freq_two_word[two_word]+=1\n",
    "    else:\n",
    "        freq_two_word[two_word]=1\n",
    "# find out probability of each bigram\n",
    "probability_two_gram = {}\n",
    "for two_gram_words in freq_two_word:\n",
    "    probability_two_gram[two_gram_words] = freq_two_word[two_gram_words]/freq_each_word[two_gram_words[0]]\n",
    "# print out probability of each bigram\n",
    "print(\"Part 2\\n\", probability_two_gram)\n",
    "print()\n",
    "\n",
    "## NOUN PRBABILITY\n",
    "# get all the noun phrases\n",
    "noun_phrases, count = \"\", 0\n",
    "for word, tag in nltk.pos_tag(clean_text): # get tags of each words\n",
    "    if re.match(r\"NN.*\", tag): # get all the nouns\n",
    "        count+=1\n",
    "        if count>=1: noun_phrases = noun_phrases + word + \" \" # add to the noun_phrase is the noun us present in the liist\n",
    "    else: noun_phrases, count = noun_phrases+\"-----\", 0\n",
    "noun_phrases = re.sub(r\"-+\",\"?\",noun_phrases).split(\"?\") # seperating the npoun pharses in\n",
    "noun_phrases = [i.strip() for i in noun_phrases if i!=\"\"]\n",
    "\n",
    "# calculte the frequency of the noun phrases\n",
    "noun_phrase_freq = {}\n",
    "for nPhrase in noun_phrases:\n",
    "    if nPhrase in noun_phrase_freq: noun_phrase_freq[nPhrase]+=1\n",
    "    else: noun_phrase_freq[nPhrase]=1\n",
    "# Calculate probality of the noun phrases\n",
    "max_freq = max(noun_phrase_freq.values()) # max noun phrase freq\n",
    "noun_phrase_probability = {}\n",
    "\n",
    "for nPhraseFreq in noun_phrase_freq:\n",
    "    noun_phrase_probability[nPhraseFreq] = noun_phrase_freq[nPhraseFreq]/max_freq # noun phrase freq\n",
    "\n",
    "nounPhraseProbability_df = pd.DataFrame(noun_phrase_probability.items(), columns=['Noun Phrase', 'Probability']) # making up dataframe of the probability\n",
    "print(\"Part 3\\n\", nounPhraseProbability_df) # print out noun phrase probabilty\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfpMRCrRwN6Z"
   },
   "source": [
    "# **Question 2: Undersand TF-IDF and Document representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dCQEbDawWCw"
   },
   "source": [
    "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
    "\n",
    "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
    "\n",
    "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vATjQNTY8buA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_3464/2016627503.py:73: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cosine = float(dot_product/(q_mod * doc_mod))\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "#Import Libraries\n",
    "import nltk, string, re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "query = \"An Outstanding movie with a haunting performance and best character development\" # Query Sentence\n",
    "\n",
    "reviews = list(pd.read_csv(\"Reviews.CSV\", nrows=500)[\"Reviews\"]) # get reviews\n",
    "total_seneteces = len(reviews) # total number of reviews\n",
    "\n",
    "def clean_txt(txt):\n",
    "    \"\"\"\n",
    "    Input: txt --> str\n",
    "    Func: Clean the given text\n",
    "    Output: list of clean text\n",
    "    \"\"\"\n",
    "    words = nltk.tokenize.word_tokenize(txt) # tokenize txt\n",
    "    stopwords = nltk.corpus.stopwords.words('english') # get all stopwords\n",
    "    punctuations = string.punctuation # get all punctuations\n",
    "    clean_text = [j.lower() for j in words if ((j not in stopwords) and (j not in punctuations) and (j.isalpha()))] # removing all punctions, stopwords, and non alphabatic words\n",
    "    return clean_text\n",
    "\n",
    "def get_td_idf(reviews):\n",
    "    \"\"\"\n",
    "    Input: reviews --> list: list of all reviews\n",
    "    Func: find tf, idf, word weight, words weight dataframe, word weight array\n",
    "    Output: list of tf, idf, word weight, words weight dataframe, word weight array\n",
    "    \"\"\"\n",
    "    tf = []\n",
    "    idf = {}\n",
    "    for review in reviews:\n",
    "        text_counter = Counter(review) # count freq of each word\n",
    "        counted_words = []\n",
    "        sentence_tf = {}\n",
    "        total_words = len(review)\n",
    "        for word in review:\n",
    "            if word not in counted_words: # go through each word of review\n",
    "                sentence_tf[word] = text_counter[word]/total_words # store tf value of each word\n",
    "                counted_words.append(word)\n",
    "            if word not in idf:  \n",
    "                count = 1\n",
    "                for sentence in reviews: # check the presence of each word in whole document\n",
    "                    if word in sentence: \n",
    "                        count+=1\n",
    "                idf[word]= np.log10(total_seneteces/count) # store idf value of each word \n",
    "        tf.append(sentence_tf)\n",
    "\n",
    "    word_weight = []\n",
    "    for sentence_tf in tf:\n",
    "        sentence_weight = {}\n",
    "        for word in sentence_tf:\n",
    "            sentence_weight[word] = sentence_tf[word] * idf[word] # Calculate sentence weight\n",
    "        word_weight.append(sentence_weight)\n",
    "\n",
    "    words_weight_dataframe = pd.DataFrame(word_weight) # make dataframe of sentence weight\n",
    "    word_weight_array = words_weight_dataframe.values\n",
    "\n",
    "    return [tf, idf, word_weight, words_weight_dataframe, word_weight_array] \n",
    "\n",
    "clean_reviews = [clean_txt(i) for i in reviews] # clean all reviews\n",
    "\n",
    "[tf, idf, word_weight, words_weight_dataframe, word_weight_array] = get_td_idf(clean_reviews)  # apply get_td_idf on all reviews\n",
    "\n",
    "q_clean = clean_txt(query)  # clean query sentence\n",
    "[q_tf, q_idf, q_word_weight, q_words_weight_dataframe, q_word_weight_array] = get_td_idf([q_clean]) # apply get_td_idf on query\n",
    "\n",
    "def cosine_similarity(tfidf_query, tfidf_reviews , query , doc_num):\n",
    "    \"\"\"\n",
    "    Func: find cosin similarity between reviews as given query\n",
    "    \"\"\"\n",
    "    dot_product = 0\n",
    "    q_mod = 0\n",
    "    doc_mod = 0\n",
    "   \n",
    "    for keyword in query: # go through each word of query\n",
    "        try: query_score = tfidf_query[0][keyword] # try to find query word if not it suppose to be zero\n",
    "        except: query_score = 0\n",
    "        try: doc_score = tfidf_reviews[doc_num][keyword] # try to find reviews word if not it suppose to be zero\n",
    "        except: doc_score=0\n",
    "        \n",
    "        dot_product += query_score * doc_score # dot product of query and review score\n",
    "        q_mod += query_score**2 # magnitude of query score\n",
    "        doc_mod += doc_score**2 # magnitude of review score\n",
    "    q_mod = np.sqrt(q_mod) \n",
    "    doc_mod = np.sqrt(doc_mod)\n",
    "    cosine = float(dot_product/(q_mod * doc_mod)) # find cosin value\n",
    "    if str(cosine)=='nan':\n",
    "        return 0\n",
    "    return cosine\n",
    "    \n",
    "\n",
    "cosin_sim = {}\n",
    "\n",
    "for n in range(len(reviews)): # go through each review and find cosin simalirty between review and query and store it in a dict\n",
    "    cosin_sim[reviews[n]] = cosine_similarity(q_word_weight, word_weight, q_clean, n)\n",
    "\n",
    "review_ranking = dict(sorted(cosin_sim.items(),key=lambda x:x[1],reverse = True)) # sort the dict\n",
    "\n",
    "review_ranking_df = pd.DataFrame(review_ranking.items(), columns=['Review', 'Cosin Similarity']) # store the data in a dataframe\n",
    "\n",
    "review_ranking_df.to_csv('Review Ranking.CSV', index = False) # save the data frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5mmYIfN8eYV"
   },
   "source": [
    "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsi2y4z88ngX"
   },
   "source": [
    "(15 points). **You dodn't need to write program for this question!** Read each of the 500 review (if you collected the data from citeseer, you can think of other text classfication tasks instead of sentiment analysis) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfvMKJjIXS5G"
   },
   "outputs": [],
   "source": [
    "# The GitHub link of your final csv file\n",
    "\n",
    "\n",
    "# Link: "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMniIalS+f3MyeuLTJeFDvi",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "INFO5731_Assignment_Three.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "11f1dc213e07634baa4c5c321dec03c05dafae643c50f20e6d1a492290c05dc2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
